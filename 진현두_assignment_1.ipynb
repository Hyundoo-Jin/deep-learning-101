{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jin/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, X, n_hiddens, n_in, n_out):\n",
    "        self.X = X\n",
    "        for i, n_hidden in enumerate(n_hiddens):\n",
    "            if i == 0:\n",
    "                input = X\n",
    "                input_dim = n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = n_hiddens[i-1]\n",
    "\n",
    "            layer_name = \"layer_\"+str(i+1)\n",
    "\n",
    "            with tf.variable_scope(layer_name, reuse=tf.AUTO_REUSE):\n",
    "                W = self._weight_variable(layer_name, [input_dim, n_hidden])\n",
    "                b = self._bias_variable(layer_name, [n_hidden])\n",
    "\n",
    "                output = tf.nn.softmax_cross_entropy_with_logits_v2(tf.matmul(input, W) + b)\n",
    "\n",
    "        layer_name = \"output\"\n",
    "        with tf.variable_scope(layer_name, reuse=tf.AUTO_REUSE):\n",
    "            W = self._weight_variable(layer_name, [n_hiddens[-1], n_out])\n",
    "            b = self._bias_variable(layer_name, [n_out])\n",
    "            self.hypothesis = tf.matmul(output, W) + b\n",
    "\n",
    "    def _weight_variable(self, layer_name, shape):\n",
    "        return tf.get_variable(\n",
    "            layer_name + \"_w\", shape=shape,\n",
    "            initializer=tf.random_normal_initializer)\n",
    "\n",
    "    def _bias_variable(self, layer_name, shape):\n",
    "        return tf.get_variable(\n",
    "            layer_name + \"_bias\", shape=shape,\n",
    "            initializer=tf.random_normal_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training dataset.\n",
      "Epoch: 0001 cost = 5.022602015\n",
      "Accuracy: 0.0072886297\n",
      "End of training dataset.\n",
      "Epoch: 0002 cost = 4.805653594\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0003 cost = 4.784485766\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0004 cost = 4.782761024\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0005 cost = 4.783287643\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0006 cost = 4.783010886\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0007 cost = 4.783306518\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0008 cost = 4.783097311\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0009 cost = 4.782759549\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0010 cost = 4.783156483\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0011 cost = 4.783147020\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0012 cost = 4.783402641\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0013 cost = 4.783111792\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0014 cost = 4.782974258\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0015 cost = 4.782776605\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0016 cost = 4.782977691\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0017 cost = 4.783735774\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0018 cost = 4.784005591\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0019 cost = 4.782563738\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0020 cost = 4.783030994\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0021 cost = 4.783393002\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0022 cost = 4.782813916\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0023 cost = 4.782981161\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0024 cost = 4.783347863\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0025 cost = 4.783440964\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0026 cost = 4.783256839\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0027 cost = 4.783041301\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0028 cost = 4.782928988\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0029 cost = 4.782781330\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0030 cost = 4.783499586\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0031 cost = 4.782974705\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0032 cost = 4.782953123\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0033 cost = 4.783130785\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0034 cost = 4.783264241\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0035 cost = 4.783394821\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0036 cost = 4.783458438\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0037 cost = 4.783142347\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0038 cost = 4.783277959\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0039 cost = 4.782924073\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0040 cost = 4.783046018\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0041 cost = 4.782719245\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0042 cost = 4.782752470\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0043 cost = 4.783203272\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0044 cost = 4.782409573\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0045 cost = 4.782605641\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0046 cost = 4.783125188\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0047 cost = 4.782356651\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0048 cost = 4.782902160\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0049 cost = 4.782311792\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0050 cost = 4.782897758\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0051 cost = 4.782915152\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0052 cost = 4.782787521\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0053 cost = 4.783135920\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0054 cost = 4.782875905\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0055 cost = 4.782810307\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0056 cost = 4.782697795\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0057 cost = 4.782740571\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0058 cost = 4.782605817\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0059 cost = 4.782896233\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0060 cost = 4.782277863\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0061 cost = 4.782505512\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0062 cost = 4.782946190\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0063 cost = 4.782760826\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0064 cost = 4.782131100\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0065 cost = 4.782649722\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0066 cost = 4.782379671\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0067 cost = 4.783164288\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0068 cost = 4.782582364\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0069 cost = 4.782409998\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0070 cost = 4.782626218\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0071 cost = 4.783072002\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0072 cost = 4.783039922\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0073 cost = 4.783064182\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0074 cost = 4.783021377\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0075 cost = 4.782495337\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0076 cost = 4.782541055\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0077 cost = 4.782609360\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0078 cost = 4.782160135\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0079 cost = 4.782526882\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0080 cost = 4.782827458\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0081 cost = 4.782769365\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0082 cost = 4.782198539\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0083 cost = 4.782579561\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0084 cost = 4.782540395\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0085 cost = 4.782041256\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0086 cost = 4.782222102\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0087 cost = 4.782427979\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0088 cost = 4.782582907\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0089 cost = 4.782697032\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0090 cost = 4.782426863\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0091 cost = 4.782709687\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0092 cost = 4.782248651\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0093 cost = 4.782728166\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0094 cost = 4.782921344\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0095 cost = 4.782613138\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0096 cost = 4.782505028\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0097 cost = 4.782422359\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0098 cost = 4.782788878\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0099 cost = 4.782641668\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0100 cost = 4.782556336\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0101 cost = 4.783186157\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0102 cost = 4.782614781\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0103 cost = 4.782552778\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0104 cost = 4.782592245\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0105 cost = 4.782458591\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0106 cost = 4.782584249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0107 cost = 4.782533257\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0108 cost = 4.782216762\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0109 cost = 4.782872097\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0110 cost = 4.782583427\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0111 cost = 4.782785027\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0112 cost = 4.782442951\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0113 cost = 4.782595246\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0114 cost = 4.782375196\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0115 cost = 4.782237897\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0116 cost = 4.782054974\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0117 cost = 4.782218669\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0118 cost = 4.782403476\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0119 cost = 4.782029247\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0120 cost = 4.782800058\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0121 cost = 4.782233407\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0122 cost = 4.782308102\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0123 cost = 4.782744034\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0124 cost = 4.783009837\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0125 cost = 4.782884260\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0126 cost = 4.783102872\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0127 cost = 4.782973165\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0128 cost = 4.782525129\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0129 cost = 4.782267152\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0130 cost = 4.782365256\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0131 cost = 4.782779041\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0132 cost = 4.782518988\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0133 cost = 4.782847500\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0134 cost = 4.782122737\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0135 cost = 4.782838102\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0136 cost = 4.782947907\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0137 cost = 4.782252238\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0138 cost = 4.782604078\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0139 cost = 4.782249832\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0140 cost = 4.782512027\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0141 cost = 4.782326398\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0142 cost = 4.781846450\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0143 cost = 4.782563613\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0144 cost = 4.782985702\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0145 cost = 4.782717507\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0146 cost = 4.783203719\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0147 cost = 4.782755668\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0148 cost = 4.782171191\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0149 cost = 4.782420034\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0150 cost = 4.782441183\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0151 cost = 4.782850236\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0152 cost = 4.782372064\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0153 cost = 4.782647580\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0154 cost = 4.782737996\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0155 cost = 4.782777339\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0156 cost = 4.782407863\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0157 cost = 4.782688955\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0158 cost = 4.782483702\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0159 cost = 4.782371961\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0160 cost = 4.782369592\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0161 cost = 4.782478090\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0162 cost = 4.782210636\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0163 cost = 4.782375857\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0164 cost = 4.782156372\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0165 cost = 4.782981968\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0166 cost = 4.782286908\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0167 cost = 4.782572622\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0168 cost = 4.782821017\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0169 cost = 4.782693966\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0170 cost = 4.782376766\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0171 cost = 4.782303627\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0172 cost = 4.782223672\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0173 cost = 4.782432226\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0174 cost = 4.782505204\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0175 cost = 4.782151633\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0176 cost = 4.782504647\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0177 cost = 4.782889102\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0178 cost = 4.782690752\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0179 cost = 4.782660990\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0180 cost = 4.782146014\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0181 cost = 4.782535868\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0182 cost = 4.782279227\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0183 cost = 4.782206631\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0184 cost = 4.782509877\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0185 cost = 4.782700671\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0186 cost = 4.781739404\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0187 cost = 4.782314616\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0188 cost = 4.782151435\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0189 cost = 4.783023240\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0190 cost = 4.782927579\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0191 cost = 4.782214737\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0192 cost = 4.782515863\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0193 cost = 4.782048299\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0194 cost = 4.782682932\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0195 cost = 4.782399031\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0196 cost = 4.782474980\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0197 cost = 4.781993521\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0198 cost = 4.781600769\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0199 cost = 4.782706173\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0200 cost = 4.782413960\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0201 cost = 4.782457953\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0202 cost = 4.782894479\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0203 cost = 4.782882272\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0204 cost = 4.782969467\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0205 cost = 4.782776282\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0206 cost = 4.782063858\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0207 cost = 4.782306803\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0208 cost = 4.782368271\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0209 cost = 4.782782364\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0210 cost = 4.782461203\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0211 cost = 4.782403176\n",
      "Accuracy: 0.011418853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training dataset.\n",
      "Epoch: 0212 cost = 4.782663331\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0213 cost = 4.782078112\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0214 cost = 4.782346997\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0215 cost = 4.782367340\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0216 cost = 4.782904940\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0217 cost = 4.782840347\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0218 cost = 4.782336470\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0219 cost = 4.781974770\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0220 cost = 4.782551773\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0221 cost = 4.782635887\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0222 cost = 4.782428734\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0223 cost = 4.782420056\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0224 cost = 4.782903987\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0225 cost = 4.782307537\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0226 cost = 4.782060322\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0227 cost = 4.782820349\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0228 cost = 4.783049554\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0229 cost = 4.781988232\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0230 cost = 4.782636026\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0231 cost = 4.782056779\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0232 cost = 4.782452642\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0233 cost = 4.782562021\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0234 cost = 4.782424538\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0235 cost = 4.782645123\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0236 cost = 4.782437743\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0237 cost = 4.782749051\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0238 cost = 4.781997395\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0239 cost = 4.782897707\n",
      "Accuracy: 0.008260447\n",
      "End of training dataset.\n",
      "Epoch: 0240 cost = 4.782192194\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0241 cost = 4.782459773\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0242 cost = 4.782616182\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0243 cost = 4.782246707\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0244 cost = 4.782833789\n",
      "Accuracy: 0.011661808\n",
      "End of training dataset.\n",
      "Epoch: 0245 cost = 4.782796610\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0246 cost = 4.782447272\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0247 cost = 4.782400256\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0248 cost = 4.782702380\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0249 cost = 4.782463059\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0250 cost = 4.782489175\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0251 cost = 4.781889703\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0252 cost = 4.782551831\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0253 cost = 4.782925510\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0254 cost = 4.782050822\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0255 cost = 4.782564772\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0256 cost = 4.782341209\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0257 cost = 4.782464541\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0258 cost = 4.782875839\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0259 cost = 4.781921893\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0260 cost = 4.782605259\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0261 cost = 4.783030642\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0262 cost = 4.782684964\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0263 cost = 4.782341458\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0264 cost = 4.782680886\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0265 cost = 4.782067137\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0266 cost = 4.782565726\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0267 cost = 4.783215332\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0268 cost = 4.782184403\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0269 cost = 4.782391533\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0270 cost = 4.782900964\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0271 cost = 4.782123170\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0272 cost = 4.782542676\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0273 cost = 4.782256669\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0274 cost = 4.782839570\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0275 cost = 4.782032145\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0276 cost = 4.782687261\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0277 cost = 4.782761618\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0278 cost = 4.782589524\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0279 cost = 4.782747269\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0280 cost = 4.782855305\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0281 cost = 4.782474349\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0282 cost = 4.782495565\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0283 cost = 4.782657308\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0284 cost = 4.782695660\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0285 cost = 4.782505035\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0286 cost = 4.783113120\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0287 cost = 4.782641037\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0288 cost = 4.781949051\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0289 cost = 4.782881385\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0290 cost = 4.782978740\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0291 cost = 4.782986414\n",
      "Accuracy: 0.008746356\n",
      "End of training dataset.\n",
      "Epoch: 0292 cost = 4.781684721\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0293 cost = 4.782457403\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0294 cost = 4.782396845\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0295 cost = 4.781987902\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0296 cost = 4.782796955\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0297 cost = 4.782446304\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0298 cost = 4.782387689\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0299 cost = 4.782694530\n",
      "Accuracy: 0.011418853\n",
      "End of training dataset.\n",
      "Epoch: 0300 cost = 4.782338810\n",
      "Accuracy: 0.011418853\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def loss_function(hypothesis, Y):\n",
    "    with tf.variable_scope(\"loss\", reuse=tf.AUTO_REUSE):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=hypothesis, labels=Y), name=\"softmax_cross_entropy\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    return train_step\n",
    "\n",
    "\n",
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        # Defaults are not specified since both keys are required.\n",
    "        features={\n",
    "          'image': tf.FixedLenFeature([], tf.string),\n",
    "          'label': tf.FixedLenFeature([], tf.string),\n",
    "      })\n",
    "\n",
    "\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    image = tf.reshape(image, [1024,]) \n",
    "    #image.set_shape((784))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    label = tf.decode_raw(features['label'], tf.uint8)\n",
    "    label.set_shape((120))\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def train(config_dict):\n",
    "    X = tf.placeholder(tf.float32, [None, 32*32])\n",
    "    Y = tf.placeholder(tf.float32, [None, 120])\n",
    "\n",
    "    n_in = 32*32\n",
    "    n_hiddens = config_dict[\"n_hiddens\"]\n",
    "    n_out = 120\n",
    "\n",
    "    mlp_model = MLP(X, n_hiddens=n_hiddens, n_in=n_in, n_out=n_out)\n",
    "    loss = loss_function(mlp_model.hypothesis, Y)\n",
    "    train_step = training(loss, learning_rate=config_dict[\"learning_rate\"])\n",
    "\n",
    "    import glob\n",
    "    filenames = glob.glob(\"./tfrecords/general/train/*.tfrecords\")\n",
    "    train_dataset = tf.data.TFRecordDataset(filenames)\n",
    "    train_dataset = train_dataset.map(decode)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.batch(config_dict[\"batch_size\"])\n",
    "    iterator = tf.data.Iterator.from_structure(\n",
    "                            train_dataset.output_types,\n",
    "                            train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "\n",
    "    filenames = glob.glob(\"./tfrecords/general/test/*.tfrecords\")\n",
    "    test_dataset = tf.data.TFRecordDataset(filenames)\n",
    "    test_dataset = test_dataset.map(decode)\n",
    "    test_dataset = test_dataset.batch(105824)\n",
    "    test_iterator = tf.data.Iterator.from_structure(\n",
    "                            test_dataset.output_types,\n",
    "                            test_dataset.output_shapes)\n",
    "    test_next_element = test_iterator.get_next()\n",
    "    test_init_op = test_iterator.make_initializer(test_dataset)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(config_dict[\"training_epochs\"]):\n",
    "            # initialize the iterator on the training data\n",
    "            sess.run(train_init_op)\n",
    "\n",
    "            # get each element of the training dataset until the end is reached\n",
    "            cost = 0\n",
    "            total_batch = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = sess.run(next_element)\n",
    "                    batch_xs = batch[0]\n",
    "                    batch_ys = batch[1]\n",
    "\n",
    "                    feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "                    c, _ = sess.run([loss, train_step], feed_dict=feed_dict)\n",
    "                    cost += c\n",
    "                    total_batch += 1\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print(\"End of training dataset.\")\n",
    "                    break\n",
    "\n",
    "            avg_cost = cost / total_batch\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(\n",
    "                    avg_cost))\n",
    "\n",
    "            sess.run(test_init_op)\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = sess.run(test_next_element)\n",
    "                    batch_x = batch[0]\n",
    "                    batch_y = batch[1]\n",
    "\n",
    "                    correct_prediction = tf.equal(\n",
    "                            tf.argmax(mlp_model.hypothesis, axis=1),\n",
    "                            tf.argmax(Y, axis=1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                    print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "                          X: batch_x, Y: batch_y}))\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_dict = {\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"n_hiddens\": [1024, 512, 256],\n",
    "        \"training_epochs\": 300,\n",
    "        \"batch_size\": 256\n",
    "        }\n",
    "\n",
    "    train(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
