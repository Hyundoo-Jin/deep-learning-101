# deep_learning_101

어떤질문들에 대답해야 할까?
- 딥러닝이 왜 좋아졌어요? -> 발전한 계기
- 딥러닝이 어떻게 발전했어요? -> 역사
- 모델을 어떻게 만들까요?
- 프레임워크간의 차이는 어떻게 되나요?
- 이미지 처리는 어떻게 하면 되나요?
- 딥러닝으로 코딩한다는 의미? 조건절 vs. 확률 프로그래밍
- 학습이 왜 잘 안되요
  - gradient vanishing
  - overfitting
  - SLOW
- Backpropagation의 기본 원리
  - SoftMax
  - Sigmoid

- LOSS functions
- Learning rate
- Optimizer
  - Stochastic GD
  - mini-batch
  - epoch의 의미?
  - visualization
  - Momentum
  - Adagrad
  - NAG
  - Nadam
  - Adam
  - RMSProp
  - AdaDelta
  - 비쥬얼라이제인션이랑 Numpy 코드를제시해주자
- Activation functions
  - ReLU
  - Sigmoid
  - Tahn
  - 근데 이게 어떻게 다른거에요? 왜 Sigmoid가 아닌 Relu를 쓰죠?
- Overfitting 극복
  - Dropout
- ImageNet 이야기
  - https://image.slidesharecdn.com/random-170910154045/95/-64-638.jpg?cb=1505089848


- CNN
  - CNN이 왜 나왔는지에 대해서 얘기해줘야 한다.
  - Stide, Padding, MaxPooling
   https://image.slidesharecdn.com/random-170910154045/95/-83-638.jpg?cb=1505089848
  - AlexNet
  - VGG
  - GoogleNet
  - ResNet
  - InceptionV3
- RNN
  - Valila RNN
  - 왜 RNN이 나왔을까?
  - LSTM 구조
  - GRU
  - Bidirectional-LSTM
  - Attention 모델
- 어떤 분석을 하고 싶어?
- 이미지 처리 - 분류, object deection
  - Data augumentation
  - 데이터 구조가 어떻게 생겼는지 얘기해줘야 함
  -
- 텍스트 처리 - 분류,
- Log 데이터 분석
- Structed data 분석하기


#To Read List
- [History and Background](https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html)
- [자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다.](https://www.slideshare.net/yongho/ss-79607172), 하용호, 2017
- [계산 그래프로 역전파 이해하기](https://brunch.co.kr/@chris-song/22)
- Word Embedding
    - [Neural Network Language Model](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/29/NNLM/), ratsgo's blog, 2017
    - [Word2Vec의 학습 방식](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/), ratsgo's blog, 2017
    - [빈도수 세기의 놀라운 마법 Word2Vec, Glove, Fasttext](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/), ratsgo's blog, 2017
# References
